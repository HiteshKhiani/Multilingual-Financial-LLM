{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers==4.40.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load sentiment classification dataset\nsentiment_df = pd.read_parquet(\"hf://datasets/FinGPT/fingpt-sentiment-cls/data/train-00000-of-00001-921f33f83a4110cb.parquet\")\n\n# Load FiQA QA dataset\nqa_df = pd.read_parquet(\"hf://datasets/FinGPT/fingpt-fiqa_qa/data/train-00000-of-00001-ab79bf9300210e98.parquet\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/AI4Bharat/IndicTrans2.git\n%%capture\n%cd /content/IndicTrans2/huggingface_interface\n%%capture\n!python3 -m pip install nltk sacremoses pandas regex mock transformers==4.53.2 mosestokenizer\n!python3 -c \"import nltk; nltk.download('punkt')\"\n!python3 -m pip install bitsandbytes scipy accelerate datasets\n!python3 -m pip install sentencepiece\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1ï¸âƒ£ Re-clone (if not already there)\n!rm -rf IndicTransToolkit\n!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n\n# 2ï¸âƒ£ Move into it and install properly (editable install breaks after restart)\n%cd IndicTransToolkit\n!pip install .\n\n# 3ï¸âƒ£ Go back to parent folder\n%cd ..\n\n# 4ï¸âƒ£ Verify installation\nimport IndicTransToolkit\nfrom IndicTransToolkit.processor import IndicProcessor\nprint(\"âœ… IndicTransToolkit imported successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\nfrom IndicTransToolkit.processor import IndicProcessor\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquantization = None  # keep as None unless you want 8-bit or 4-bit loading\nBATCH_SIZE = 4       # adjust based on GPU memory\n\n# âœ… Use the rotary model with 2048-token context\nen_indic_ckpt_dir = \"prajdabre/rotary-indictrans2-en-indic-dist-200M\"\n\ndef initialize_model_and_tokenizer(ckpt_dir, quantization):\n    if quantization == \"4-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n    elif quantization == \"8-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_8bit=True,\n            bnb_8bit_use_double_quant=True,\n            bnb_8bit_compute_dtype=torch.bfloat16,\n        )\n    else:\n        qconfig = None\n\n    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        ckpt_dir,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        quantization_config=qconfig,\n        torch_dtype=torch.float16,\n    )\n\n    if qconfig is None:\n        model = model.to(DEVICE)\n\n    model.eval()\n    return tokenizer, model\n\n\ndef batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n    translations = []\n    for i in range(0, len(input_sentences), BATCH_SIZE):\n        batch = input_sentences[i: i + BATCH_SIZE]\n\n        # Preprocess\n        preprocessed = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n\n        # âœ… Extended token length (2048)\n        inputs = tokenizer(\n            preprocessed,\n            truncation=True,\n            max_length=1024,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n        ).to(DEVICE)\n\n        with torch.no_grad():\n            generated_tokens = model.generate(\n                **inputs,\n                use_cache=True,\n                max_new_tokens=512,\n                num_beams=3,           # a bit higher for better fluency\n                early_stopping=True,\n            )\n\n        decoded = tokenizer.batch_decode(\n            generated_tokens,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n\n        postprocessed = ip.postprocess_batch(decoded, lang=tgt_lang)\n        translations.extend(postprocessed)\n\n        del inputs\n        torch.cuda.empty_cache()\n\n    return translations\n\n\n# âœ… Initialize model and processor\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)\nip = IndicProcessor(inference=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Number of rows to preview\nNUM_SAMPLES = 3  \n\ndef inspect_translations(df, src_lang, tgt_lang, model, tokenizer, ip, name):\n    \"\"\"\n    Translates first few samples from a dataset's columns (instruction, input, output)\n    and prints side-by-side English â†’ Hindi translations.\n    \"\"\"\n    print(f\"\\n\\n=== {name} Dataset Translations ({src_lang} â†’ {tgt_lang}) ===\")\n    \n    # Select top samples\n    df_sample = df.head(NUM_SAMPLES).copy()\n    \n    # Columns to translate\n    columns_to_translate = [\"instruction\", \"input\", \"output\"]\n\n    for col in columns_to_translate:\n        print(f\"\\nðŸ”¹ Translating column: {col} ...\")\n        texts = df_sample[col].fillna(\"\").astype(str).tolist()\n        \n        # Perform translation using your batch_translate() from earlier\n        translated_texts = batch_translate(\n            texts,\n            src_lang=src_lang,\n            tgt_lang=tgt_lang,\n            model=model,\n            tokenizer=tokenizer,\n            ip=ip\n        )\n        df_sample[f\"{col}_hi\"] = translated_texts  # Store Hindi translations\n\n    # --- Display results nicely ---\n    for i, row in df_sample.iterrows():\n        print(\"\\n\" + \"=\" * 100)\n        print(f\"ðŸ“ Row {i+1}\")\n\n        for col in columns_to_translate:\n            print(f\"\\n{col.capitalize()}:\")\n            print(f\"EN: {row[col]}\")\n            print(f\"HI: {row[f'{col}_hi']}\")\n        print(\"=\" * 100)\n\n# Example usage:\ninspect_translations(\n    sentiment_df, \"eng_Latn\", \"hin_Deva\",\n    en_indic_model, en_indic_tokenizer, ip, name=\"Sentiment\"\n)\n\ninspect_translations(\n    qa_df, \"eng_Latn\", \"hin_Deva\",\n    en_indic_model, en_indic_tokenizer, ip, name=\"QA\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n# Use a generic English tokenizer for counting (not IndicTrans)\ncount_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\nmax_len = 1024\ntoken_counts = []\n\n# Count tokens safely\nfor text in tqdm(qa_df[\"output\"].fillna(\"\").astype(str).tolist(), desc=\"Counting tokens\"):\n    tokens = count_tokenizer.encode(\n        text,\n        add_special_tokens=False,\n        truncation=False\n    )\n    token_counts.append(len(tokens))\n\nqa_df[\"output_token_count\"] = token_counts\n\n# --- Stats ---\ntotal_rows = len(qa_df)\ntoo_long = qa_df[qa_df[\"output_token_count\"] > max_len]\nmax_tokens = qa_df[\"output_token_count\"].max()\n\nprint(f\"Total rows: {total_rows}\")\nprint(f\"Rows that would exceed {max_len} tokens: {len(too_long)}\")\nprint(f\"Percentage: {(len(too_long) / total_rows * 100):.2f}%\")\nprint(f\"Maximum token length in 'output': {max_tokens}\")\n\n# Optional: inspect a few long examples\ndisplay(too_long[[\"output\", \"output_token_count\"]].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the max token limit\nmax_len = 1024\n\n# Filter rows with acceptable token count\nqa_df_filtered = qa_df[qa_df[\"output_token_count\"] <= max_len].copy()\n\n# Report how many rows were removed\nremoved_rows = len(qa_df) - len(qa_df_filtered)\nprint(f\"âœ… Removed {removed_rows} rows exceeding {max_len} tokens.\")\nprint(f\"Remaining rows: {len(qa_df_filtered)}\")\n\n# (Optional) Save the filtered dataframe for safe use\nqa_df_filtered.to_csv(\"qa_df_filtered.csv\", index=False)\nprint(\"ðŸ’¾ Saved filtered dataframe as 'qa_df_filtered.csv'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os\nfrom tqdm import tqdm\n\n# ================== CONFIG ==================\nINPUT_CSV = \"qa_df_filtered.csv\"       # your source file\nSAVE_CSV = \"translated_dataset.csv\"    # output file\nPROGRESS_FILE = \"translation_progress.json\"\n\nSRC_LANG = \"eng_Latn\"\nTGT_LANG = \"hin_Deva\"\n\nCHUNK_SIZE = 1000   # save every 1000 rows\nBATCH_SIZE = 32     # already set above\n\n\n# ================== RESUMABLE TRANSLATION (AUTO-SKIP) ==================\ndef translate_large_csv_safe(input_csv, output_csv, progress_file, src_lang, tgt_lang, model, tokenizer, ip):\n    # --- Step 1: Load input dataset ---\n    df = pd.read_csv(input_csv)\n    print(f\"Loaded {len(df)} rows from {input_csv}\")\n\n    # --- Step 2: Load existing translated data if present ---\n    if os.path.exists(output_csv):\n        translated_df = pd.read_csv(output_csv)\n        done_rows = len(translated_df)\n        print(f\"Found existing translated CSV with {done_rows} rows âœ…\")\n    else:\n        translated_df = pd.DataFrame()\n        done_rows = 0\n\n    # --- Step 3: Resume from JSON progress if available ---\n    start_idx = 0\n    if os.path.exists(progress_file):\n        with open(progress_file, \"r\") as f:\n            progress = json.load(f)\n            start_idx = progress.get(\"last_index\", done_rows)\n        print(f\"Resuming from row {start_idx} ðŸ”\")\n    else:\n        start_idx = done_rows\n\n    # --- Step 4: Only process un-translated rows ---\n    if start_idx >= len(df):\n        print(\"ðŸŽ‰ All rows already translated â€” nothing to do!\")\n        return\n\n    remaining_df = df.iloc[start_idx:].copy()\n\n    # --- Step 5: Process and append chunks ---\n    mode = \"a\" if os.path.exists(output_csv) else \"w\"\n    header = not os.path.exists(output_csv)\n\n    for start in range(0, len(remaining_df), CHUNK_SIZE):\n        end = min(start + CHUNK_SIZE, len(remaining_df))\n        chunk_df = remaining_df.iloc[start:end].copy()\n\n        print(f\"\\nðŸ”¹ Translating rows {start_idx + start}â€“{start_idx + end} ...\")\n\n        for col in [\"instruction\", \"input\", \"output\"]:\n            if col in chunk_df.columns:\n                texts = chunk_df[col].fillna(\"\").astype(str).tolist()\n                chunk_df[f\"{col}_hi\"] = batch_translate(\n                    texts, src_lang, tgt_lang, model, tokenizer, ip\n                )\n\n        # --- Step 6: Save incremental results ---\n        chunk_df.to_csv(output_csv, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n        mode, header = \"a\", False  # only first write includes header\n\n        # --- Step 7: Update progress ---\n        with open(progress_file, \"w\") as f:\n            json.dump({\"last_index\": start_idx + end}, f)\n\n        print(f\"âœ… Saved progress at row {start_idx + end}\")\n\n        # Free up memory\n        del chunk_df\n        import torch\n        torch.cuda.empty_cache()\n\n    print(\"\\nðŸŽ‰ Translation fully complete and saved safely to CSV!\")\n\n\n# ================== RUN TRANSLATION ==================\ntranslate_large_csv_safe(\n    INPUT_CSV, SAVE_CSV, PROGRESS_FILE,\n    SRC_LANG, TGT_LANG, model, tokenizer, ip\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os\nfrom tqdm import tqdm\nimport torch\nmodel = en_indic_model\ntokenizer = en_indic_tokenizer\n\n# ================== CONFIG ==================\nINPUT_PARQUET = \"hf://datasets/FinGPT/fingpt-sentiment-cls/data/train-00000-of-00001-921f33f83a4110cb.parquet\"\nTEMP_CSV = \"sentiment_df.csv\"             # temporary CSV for streaming\nSAVE_CSV = \"sentiment_translated.csv\"     # final translated dataset\nPROGRESS_FILE = \"sentiment_translation_progress.json\"\n\nSRC_LANG = \"eng_Latn\"\nTGT_LANG = \"hin_Deva\"\n\nCHUNK_SIZE = 1000\nBATCH_SIZE = 32\n\n\n# ================== STEP 1: Load parquet file ==================\nprint(\"ðŸ“¥ Loading sentiment dataset from HuggingFace Parquet...\")\nsentiment_df = pd.read_parquet(INPUT_PARQUET)\nprint(f\"âœ… Loaded {len(sentiment_df)} rows.\")\n\n# Save as CSV once for incremental translation process\nsentiment_df.to_csv(TEMP_CSV, index=False)\nprint(f\"ðŸ’¾ Saved a temporary working copy: {TEMP_CSV}\")\n\n\n# ================== STEP 2: Resumable translation function ==================\ndef translate_large_csv_safe(input_csv, output_csv, progress_file, src_lang, tgt_lang, model, tokenizer, ip):\n    df = pd.read_csv(input_csv)\n    print(f\"Loaded {len(df)} rows from {input_csv}\")\n\n    # --- Step 2: Resume if previous results exist ---\n    if os.path.exists(output_csv):\n        translated_df = pd.read_csv(output_csv)\n        done_rows = len(translated_df)\n        print(f\"Found existing partial output: {done_rows} rows âœ…\")\n    else:\n        translated_df = pd.DataFrame()\n        done_rows = 0\n\n    # --- Step 3: Resume from JSON progress ---\n    start_idx = 0\n    if os.path.exists(progress_file):\n        with open(progress_file, \"r\") as f:\n            progress = json.load(f)\n            start_idx = progress.get(\"last_index\", done_rows)\n        print(f\"Resuming from row {start_idx} ðŸ”\")\n    else:\n        start_idx = done_rows\n\n    # --- Step 4: Process un-translated rows only ---\n    if start_idx >= len(df):\n        print(\"ðŸŽ‰ All rows already translated â€” nothing to do!\")\n        return\n\n    remaining_df = df.iloc[start_idx:].copy()\n    mode = \"a\" if os.path.exists(output_csv) else \"w\"\n    header = not os.path.exists(output_csv)\n\n    # --- Step 5: Translate in chunks ---\n    for start in range(0, len(remaining_df), CHUNK_SIZE):\n        end = min(start + CHUNK_SIZE, len(remaining_df))\n        chunk_df = remaining_df.iloc[start:end].copy()\n        print(f\"\\nðŸ”¹ Translating rows {start_idx + start}â€“{start_idx + end} ...\")\n\n        for col in [\"instruction\", \"input\", \"output\"]:\n            if col in chunk_df.columns:\n                texts = chunk_df[col].fillna(\"\").astype(str).tolist()\n                chunk_df[f\"{col}_hi\"] = batch_translate(\n                    texts, src_lang, tgt_lang, model, tokenizer, ip\n                )\n\n        # Save progress to CSV\n        chunk_df.to_csv(output_csv, mode=mode, header=header, index=False, encoding=\"utf-8-sig\")\n        mode, header = \"a\", False\n\n        # Save checkpoint\n        with open(progress_file, \"w\") as f:\n            json.dump({\"last_index\": start_idx + end}, f)\n\n        print(f\"âœ… Saved progress at row {start_idx + end}\")\n        del chunk_df\n        torch.cuda.empty_cache()\n\n    print(\"\\nðŸŽ‰ Translation complete and safely saved to:\", output_csv)\n\n\n# ================== STEP 3: Run translation ==================\ntranslate_large_csv_safe(\n    TEMP_CSV, SAVE_CSV, PROGRESS_FILE,\n    SRC_LANG, TGT_LANG, model, tokenizer, ip\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}