{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13795432,"sourceType":"datasetVersion","datasetId":8783077},{"sourceId":13796014,"sourceType":"datasetVersion","datasetId":8783516},{"sourceId":13796036,"sourceType":"datasetVersion","datasetId":8783530},{"sourceId":13796070,"sourceType":"datasetVersion","datasetId":8783551},{"sourceId":13835264,"sourceType":"datasetVersion","datasetId":8811337},{"sourceId":13836829,"sourceType":"datasetVersion","datasetId":8812437},{"sourceId":13866452,"sourceType":"datasetVersion","datasetId":8834613}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =======================\n# W&B LOGIN FOR KAGGLE\n# =======================\nimport os\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\n# Get secret stored in Kaggle\nsecret = \"3bd2ca478538bcae330cb53163009f2ad5e48c7d\"\n\n\n# Export to environment (required by wandb)\nos.environ[\"WANDB_API_KEY\"] = secret\n\n# Silent login (no prompt)\nwandb.login(key=secret)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:05:33.861101Z","iopub.execute_input":"2025-11-25T09:05:33.861399Z","iopub.status.idle":"2025-11-25T09:05:43.733747Z","shell.execute_reply.started":"2025-11-25T09:05:33.861378Z","shell.execute_reply":"2025-11-25T09:05:43.733135Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miamharshmehtafirst\u001b[0m (\u001b[33miamharshmehtafirst-iit-bombay\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y accelerate -q\n!pip install -q \"accelerate>=0.27.0\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:05:47.854127Z","iopub.execute_input":"2025-11-25T09:05:47.855071Z","iopub.status.idle":"2025-11-25T09:06:59.411745Z","shell.execute_reply.started":"2025-11-25T09:05:47.855044Z","shell.execute_reply":"2025-11-25T09:06:59.411025Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import accelerate\nprint(accelerate.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:07:03.186364Z","iopub.execute_input":"2025-11-25T09:07:03.186688Z","iopub.status.idle":"2025-11-25T09:07:07.045821Z","shell.execute_reply.started":"2025-11-25T09:07:03.186657Z","shell.execute_reply":"2025-11-25T09:07:07.045179Z"}},"outputs":[{"name":"stdout","text":"1.12.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datasets import DatasetDict, load_dataset\nimport pandas as pd\n\nprint(f\"\\n{'='*80}\")\nprint(f\"LOADING DATASETS + CREATING TEST SPLIT\")\nprint(f\"{'='*80}\")\n\n# Load original splits\ndataset = load_dataset('json', data_files={\n    'train': '/kaggle/input/final-dataset-cs772/train_hindi.json',\n    'validation': '/kaggle/input/final-dataset-cs772/val_hindi.json'\n})\n\nprint(f\"Original split:\")\nprint(f\"  Train: {len(dataset['train']):,}\")\nprint(f\"  Validation: {len(dataset['validation']):,}\")\n\n# â¤ Split train into new_train + test (around 7% test size)\ntrain_test = dataset['train'].train_test_split(test_size=0.07, seed=42)\n\n# Build final dataset dictionary\nfinal_dataset = DatasetDict({\n    \"train\": train_test[\"train\"].shuffle(seed=42),\n    \"validation\": dataset[\"validation\"].shuffle(seed=42),\n    \"test\": train_test[\"test\"].shuffle(seed=42)\n})\n\nprint(f\"\\nâœ“ Final dataset sizes:\")\nprint(f\"  Train: {len(final_dataset['train']):,}\")\nprint(f\"  Validation: {len(final_dataset['validation']):,}\")\nprint(f\"  Test: {len(final_dataset['test']):,}\")\n\n# =============================\n# SAVE TEST SET AS CSV\n# =============================\ntest_df = pd.DataFrame(final_dataset[\"test\"])\ntest_df.to_csv(\"test_dataset_hindi.csv\", index=False)\n\nprint(\"\\nğŸ“ Test set saved as:  test_dataset_hindi.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:07:10.190748Z","iopub.execute_input":"2025-11-25T09:07:10.191201Z","iopub.status.idle":"2025-11-25T09:07:13.964969Z","shell.execute_reply.started":"2025-11-25T09:07:10.191177Z","shell.execute_reply":"2025-11-25T09:07:13.964329Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADING DATASETS + CREATING TEST SPLIT\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f25e769036c443b59f1589d14d8cfe4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25d278406bd4dd59eecef9ecf1d2f5a"}},"metadata":{}},{"name":"stdout","text":"Original split:\n  Train: 41,509\n  Validation: 5,188\n\nâœ“ Final dataset sizes:\n  Train: 38,603\n  Validation: 5,188\n  Test: 2,906\n\nğŸ“ Test set saved as:  test_dataset_hindi.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# FINGPT HINDI FINE-TUNING SCRIPT\n# Optimized for Kaggle P100 GPU (16GB)\n# Works with combined dataset (sentiment + QA)\n# ============================================================================\n\n# ============================================================================\n# CELL 0: Create Config File (if missing)\n# ============================================================================\n\nimport json\nimport os\n\nif not os.path.exists('dataset_config.json'):\n    print(\"Creating dataset_config.json...\")\n    config = {\n        'max_length': 1024,\n        'train_samples': 'Will be calculated',\n        'val_samples': 'Will be calculated'\n    }\n    with open('dataset_config.json', 'w') as f:\n        json.dump(config, f, indent=2)\n    print(\"âœ“ Created dataset_config.json with MAX_LENGTH=1536\")\nelse:\n    print(\"âœ“ dataset_config.json already exists\")\n\n# ============================================================================\n# CELL 0: System Check\n# ============================================================================\nimport subprocess\nimport sys\n\nprint(\"System Information:\")\nprint(f\"Python: {sys.version}\")\nprint(f\"Platform: {sys.platform}\")\n\n# Check CUDA\nresult = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\nif result.returncode == 0:\n    print(f\"âœ“ CUDA available\")\nelse:\n    print(\"âš ï¸ CUDA check failed\")\n\n# ============================================================================\n# CELL 1: Install Dependencies with Proper CUDA Support\n# ============================================================================\n\nimport torch\nimport subprocess\nimport os\n\n# Check CUDA version\ncuda_version = torch.version.cuda\nprint(f\"Detected CUDA version: {cuda_version}\")\n\n# Install protobuf first\n!pip uninstall -y protobuf -q\n!pip install -q protobuf==3.20.3\n\n# Install core packages\n# Install latest transformers instead of 4.36.0\n!pip uninstall -y transformers -q\n!pip install -q \"transformers>=4.45.0\"\n\n!pip install -q datasets==2.16.0\n!pip install -q peft==0.7.1\n# !pip install -q accelerate==0.25.0\n!pip install -q sentencepiece==0.1.99\n\n# Uninstall any existing bitsandbytes (clean slate)\n!pip uninstall -y bitsandbytes bitsandbytes-cuda118 -q\n\nprint(\"\\nInstalling bitsandbytes with CUDA support...\")\n\n# # Install official bitsandbytes\n!pip install -q bitsandbytes\n\nprint(\"âœ“ bitsandbytes installed\")\n\n\n# If above fails, try:\n# !pip install -q bitsandbytes\n\nprint(\"âœ“ Installation complete\")\n\n# Set environment variables for CUDA\nos.environ['BNB_CUDA_VERSION'] = cuda_version.replace('.', '')\nos.environ['LD_LIBRARY_PATH'] = f\"/usr/local/cuda-{cuda_version}/lib64:\" + os.environ.get('LD_LIBRARY_PATH', '')\n\nprint(f\"\\nâœ“ CUDA environment configured\")\nprint(f\"  BNB_CUDA_VERSION: {os.environ.get('BNB_CUDA_VERSION')}\")\n\n# ============================================================================\n# CELL 2: Import Libraries\n# ============================================================================\n\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForSeq2Seq,\n    BitsAndBytesConfig\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport json\nimport os\nimport gc\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# ============================================================================\n# CELL 3: Configuration\n# ============================================================================\n\n# Configuration\nMODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nOUTPUT_DIR = \"./fingpt-hindi-tinyllama\"\n\n\n# Try to load config if it exists, otherwise use defaults\ntry:\n    with open('dataset_config.json', 'r', encoding='utf-8') as f:\n        dataset_config = json.load(f)\n    # OVERRIDE for fast TinyLlama training\n    MAX_LENGTH = 512\n    dataset_config['max_length'] = MAX_LENGTH\n    print(\"âœ“ Loaded configuration from dataset_config.json (OVERRIDDEN)\")\nexcept FileNotFoundError:\n    print(\"âš ï¸ dataset_config.json not found. Using default MAX_LENGTH\")\n    MAX_LENGTH = 512\n    dataset_config = {\n        'max_length': MAX_LENGTH,\n        'train_samples': 'Unknown - will be calculated',\n        'val_samples': 'Unknown - will be calculated'\n    }\n\nprint(f\"\\n{'='*80}\")\nprint(f\"CONFIGURATION\")\nprint(f\"{'='*80}\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Max sequence length: {MAX_LENGTH}\")\n\n\n# # ============================================================================\n# # CELL 4: Authenticate HuggingFace\n# # ============================================================================\n\n# Set your HuggingFace token\n# Get it from: https://huggingface.co/settings/tokens\nHF_TOKEN = \"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\"  # REPLACE THIS!\n\n# Or use Kaggle secrets (recommended)\nfrom kaggle_secrets import UserSecretsClient\ntry:\n    user_secrets = UserSecretsClient()\n    HF_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n    print(\"\\nâœ“ Loaded HF token from Kaggle secrets\")\nexcept:\n    print(\"\\nâš ï¸  Using hardcoded token (not recommended for production)\")\n\nos.environ[\"HF_TOKEN\"] = HF_TOKEN\n\n# ============================================================================\n# CELL 5: Setup Quantization Config\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"SETTING UP 4-BIT QUANTIZATION\")\nprint(f\"{'='*80}\")\n\n# QLoRA configuration for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16   \n)\n\n\nprint(\"âœ“ Quantization config created\")\nprint(f\"  - 4-bit precision (NF4)\")\nprint(f\"  - Double quantization enabled\")\nprint(f\"  - Compute dtype: bfloat16\")\n\n# ============================================================================\n# CELL 6: Load Tokenizer\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"LOADING TOKENIZER\")\nprint(f\"{'='*80}\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME,\n    token=HF_TOKEN,\n    trust_remote_code=True\n)\n\n# Configure tokenizer\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"âœ“ Tokenizer loaded\")\nprint(f\"  - Vocabulary size: {len(tokenizer):,}\")\nprint(f\"  - Pad token: {tokenizer.pad_token}\")\nprint(f\"  - EOS token: {tokenizer.eos_token}\")\n\n# ============================================================================\n# CELL 7: Load Model with Quantization (Single GPU) - FIXED\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"LOADING BASE MODEL (Single GPU)\")\nprint(f\"{'='*80}\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    token=HF_TOKEN,\n    trust_remote_code=True,\n    device_map=\"auto\",  # âœ… ADD THIS - handles device placement automatically\n)\n\nprint(f\"âœ“ Model loaded successfully\")\nif torch.cuda.is_available():\n    print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n    props = torch.cuda.get_device_properties(0)\n    print(f\"  - Memory: {props.total_memory / 1e9:.2f} GB\")\n\n# ============================================================================\n# CELL 8: Prepare Model for Training - FIXED\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"PREPARING MODEL FOR TRAINING\")\nprint(f\"{'='*80}\")\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# âœ… DISABLE use_cache for gradient checkpointing compatibility\nmodel.config.use_cache = False\n\nprint(\"âœ“ Model prepared for k-bit training\")\nprint(\"âœ“ use_cache disabled for gradient checkpointing\")\n\n# ============================================================================\n# CELL 9: Configure LoRA\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"CONFIGURING LORA ADAPTERS\")\nprint(f\"{'='*80}\")\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=8,                    # lower rank â†’ fewer params, faster\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"v_proj\",\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n\n# Apply LoRA to model\nmodel = get_peft_model(model, lora_config)\n\nprint(\"âœ“ LoRA adapters configured\")\nmodel.print_trainable_parameters()\n\n# ============================================================================\n# CELL 10: Data Processing Functions\n# ============================================================================\n# ============================================================================\n# CELL 10: Data Processing Functions (UPDATED FOR HINDI KEYS)\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"SETTING UP DATA PROCESSING\")\nprint(f\"{'='*80}\")\n\ndef format_prompt(example):\n    \"\"\"Format data into instruction-following format using Hindi fields\"\"\"\n    instruction = example['instruction_hi']\n    inp = example['input_hi']\n    output = example['output_hi']\n    \n    if inp and inp.strip():\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{inp}\n\n### Response:\n{output}\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Response:\n{output}\"\"\"\n    return prompt\n\ndef tokenize_function(examples):\n    prompts = []\n    \n    # Format batch\n    for inst, inp, out in zip(examples['instruction_hi'], examples['input_hi'], examples['output_hi']):\n        if inp and inp.strip():\n            prompt = f\"\"\"### Instruction:\n{inst}\n\n### Input:\n{inp}\n\n### Response:\n{out}\"\"\"\n        else:\n            prompt = f\"\"\"### Instruction:\n{inst}\n\n### Response:\n{out}\"\"\"\n        prompts.append(prompt)\n\n    # Tokenize the entire batch\n    tokenized = tokenizer(\n        prompts,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\"\n    )\n    \n    # â— labels MUST be a copy of input_ids, NOT clone()\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n\n    return tokenized\n\nprint(\"âœ“ Data processing functions defined (using instruction_hi/input_hi/output_hi)\")\n\n# ============================================================================\n# REPLACE CELL 11: Load SUBSET of data for fast training\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"LOADING DATASETS (FAST TRAINING MODE)\")\nprint(f\"{'='*80}\")\n\n# # Load full datasets\n# full_dataset = load_dataset('json', data_files={\n#     'train': '/kaggle/input/final-dataset-cs772/train_hindi.json',\n#     'validation': '/kaggle/input/final-dataset-cs772/val_hindi.json'\n# })\n\n# print(f\"Full dataset sizes:\")\n# print(f\"  Train: {len(full_dataset['train']):,}\")\n# print(f\"  Validation: {len(full_dataset['validation']):,}\")\n\n# # ğŸ”¥ TRAIN ON 5,000 SAMPLES (10-15% of data) FOR QUICK ITERATION\n# TRAIN_SIZE = 1000\n# VAL_SIZE = 200\n\n# train_subset = full_dataset['train'].shuffle(seed=42).select(range(min(TRAIN_SIZE, len(full_dataset['train']))))\n# val_subset = full_dataset['validation'].shuffle(seed=42).select(range(min(VAL_SIZE, len(full_dataset['validation']))))\n\ntrain_subset = final_dataset[\"train\"]\nval_subset   = final_dataset[\"validation\"]\ntest_subset  = final_dataset[\"test\"]\n\n\n\nprint(f\"\\nâœ“ Using subset for fast training:\")\nprint(f\"  - Train: {len(train_subset):,} samples\")\nprint(f\"  - Validation: {len(val_subset):,} samples\")\n# print(f\"  - Percentage: {len(train_subset)/len(full_dataset['train'])*100:.1f}% of data\")\n\n# Tokenize subsets\nprint(f\"\\nTokenizing datasets...\")\n\ntokenized_train = train_subset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=100,\n    remove_columns=train_subset.column_names,\n    desc=\"Tokenizing train set\"\n)\n\ntokenized_eval = val_subset.map(\n    tokenize_function,\n    batched=True,\n    batch_size=100,\n    remove_columns=val_subset.column_names,\n    desc=\"Tokenizing validation set\"\n)\n\nprint(f\"âœ“ Tokenization complete\")\n\n# Calculate new training time\neffective_batch = 4 * 8  # 32\nsteps_per_epoch = len(tokenized_train) // effective_batch\ntotal_steps = steps_per_epoch * 3\nprint(f\"\\nğŸ“Š Training estimates:\")\nprint(f\"  - Steps per epoch: {steps_per_epoch}\")\nprint(f\"  - Total steps: {total_steps}\")\nprint(f\"  - Estimated time on T4: 2-3 hours â°\")\n\n\n# ============================================================================\n# REPLACE CELL 12: Optimized training args for T4\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"CONFIGURING TRAINING (Optimized for T4)\")\nprint(f\"{'='*80}\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=True,\n    logging_steps=50,              # you had 50 â†’ OK\n    evaluation_strategy=\"epoch\",   # evaluate per epoch\n    save_strategy=\"steps\",         # <-- save by steps\n    save_steps=500,                # <-- change this to how often you want a checkpoint (e.g. 250, 500)\n    save_total_limit=3,            # keep only last 3 checkpoints\n    load_best_model_at_end=False,\n    warmup_steps=0,\n    lr_scheduler_type=\"cosine\",\n    report_to=[\"wandb\"],\n    optim=\"paged_adamw_8bit\",\n    gradient_checkpointing=False,\n    max_grad_norm=1.0,\n)\n\n\neffective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\nsteps_per_epoch = len(tokenized_train) // effective_batch_size\ntotal_steps = steps_per_epoch * training_args.num_train_epochs\n\nprint(f\"âœ“ Training configuration:\")\nprint(f\"  - Batch size per device: {training_args.per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  - Effective batch size: {effective_batch_size}\")\nprint(f\"  - Steps per epoch: {steps_per_epoch:,}\")\nprint(f\"  - Total steps: {total_steps:,}\")\n\n\n\n# ===========================================================================\n# ============================================================================\n# CELL 13: Initialize Trainer\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"INITIALIZING TRAINER\")\nprint(f\"{'='*80}\")\n\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_eval,\n    data_collator=data_collator,\n)\n\nprint(f\"âœ“ Trainer initialized\")\nprint(f\"  - Ready to train!\")\n\n\n# ============================================================================\n# CELL 14: Start Training - FIXED\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"STARTING TRAINING\")\nprint(f\"{'='*80}\")\nprint(f\"\\nâ° Estimated time: 8-12 hours on Kaggle P100\")\nprint(f\"ğŸ“Š Monitor training loss in the output below\\n\")\n\n# Clear cache before training\ngc.collect()\ntorch.cuda.empty_cache()\n\n# âœ… Ensure model is in training mode\nmodel.train()\n\n# Start training\ntry:\n    trainer.train()\n    print(f\"\\n{'='*80}\")\n    print(f\"âœ… TRAINING COMPLETE!\")\n    print(f\"{'='*80}\")\nexcept Exception as e:\n    print(f\"\\nâŒ Training failed with error:\")\n    print(f\"{type(e).__name__}: {str(e)}\")\n    import traceback\n    traceback.print_exc()\n    raise\n# ============================================================================\n# CELL 15: Save Model\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"SAVING MODEL\")\nprint(f\"{'='*80}\")\n\n# Save model\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(f\"âœ“ Model saved to {OUTPUT_DIR}\")\nprint(f\"  - adapter_config.json\")\nprint(f\"  - adapter_model.bin (~150MB)\")\nprint(f\"  - tokenizer files\")\n\nlogs = trainer.state.log_history\n\n# Find last train loss, if any\nlast_train_loss = None\nfor log in reversed(logs):\n    if 'loss' in log:\n        last_train_loss = log['loss']\n        break\n\n# Find last eval loss, if any\nlast_eval_loss = None\nfor log in reversed(logs):\n    if 'eval_loss' in log:\n        last_eval_loss = log['eval_loss']\n        break\n\ntraining_info = {\n    'model_name': MODEL_NAME,\n    'max_length': MAX_LENGTH,\n    'lora_r': 8,          # or 16, whatever you actually used\n    'lora_alpha': 16,\n    'training_samples': len(tokenized_train),\n    'validation_samples': len(tokenized_eval),\n    'epochs': training_args.num_train_epochs,\n    'final_train_loss': last_train_loss,\n    'final_eval_loss': last_eval_loss,\n}\n\nwith open(f'{OUTPUT_DIR}/training_info.json', 'w') as f:\n    json.dump(training_info, f, indent=2)\n\nprint(f\"âœ“ Training info saved\")\n\n\n# ============================================================================\n# CELL 16: Test Inference\n# ============================================================================\n\nprint(f\"\\n{'='*80}\")\nprint(f\"TESTING MODEL\")\nprint(f\"{'='*80}\")\n\ndef test_model(instruction, input_text=\"\", max_new_tokens=526):\n    \"\"\"Test the fine-tuned model\"\"\"\n    if input_text:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\n    # Get model device from parameters\n    device = next(model.parameters()).device\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = response.split(\"### Response:\")[-1].strip()\n    return response\n\n\n# Test with sentiment example\nprint(\"\\nğŸ“ Test 1: Sentiment Analysis\")\nprint(\"-\" * 80)\ntest_instruction = \"à¤¨à¤¿à¤®à¥à¤¨à¤²à¤¿à¤–à¤¿à¤¤ à¤µà¤¿à¤•à¤²à¥à¤ªà¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤Ÿà¥à¤µà¥€à¤Ÿ à¤•à¥€ à¤­à¤¾à¤µà¤¨à¤¾ à¤•à¥‹ à¤šà¤¿à¤¹à¥à¤¨à¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤ à¤µà¤¿à¤•à¤²à¥à¤ª: à¤¨à¤•à¤¾à¤°à¤¾à¤¤à¥à¤®à¤•, à¤¸à¤•à¤¾à¤°à¤¾à¤¤à¥à¤®à¤•\"\ntest_input = \"à¤•à¤‚à¤ªà¤¨à¥€ à¤•à¥€ à¤¤à¤¿à¤®à¤¾à¤¹à¥€ à¤†à¤¯ à¤‰à¤®à¥à¤®à¥€à¤¦à¥‹à¤‚ à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤°à¤¹à¥€ à¤”à¤° à¤¶à¥‡à¤¯à¤° à¤®à¥‚à¤²à¥à¤¯ à¤®à¥‡à¤‚ à¤µà¥ƒà¤¦à¥à¤§à¤¿ à¤¹à¥à¤ˆ\"\nresult = test_model(test_instruction, test_input, max_new_tokens=50)\nprint(f\"Input: {test_input}\")\nprint(f\"Output: {result}\")\n\n# Test with QA example\nprint(\"\\nğŸ“ Test 2: Question Answering\")\nprint(\"-\" * 80)\ntest_instruction = \"à¤…à¤ªà¤¨à¥‡ à¤µà¤¿à¤¤à¥à¤¤à¥€à¤¯ à¤œà¥à¤à¤¾à¤¨ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚, à¤‡à¤¨à¤ªà¥à¤Ÿ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤¯à¤¾ à¤µà¤¿à¤·à¤¯ à¤•à¥‹ à¤…à¤ªà¤¨à¤¾ à¤‰à¤¤à¥à¤¤à¤° à¤¯à¤¾ à¤°à¤¾à¤¯ à¤¦à¥‡à¤‚à¥¤ à¤‰à¤¤à¥à¤¤à¤° à¤ªà¥à¤°à¤¾à¤°à¥‚à¤ª à¤¸à¥€à¤®à¤¿à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¥¤\"\ntest_input = \"à¤¶à¥‡à¤¯à¤° à¤¬à¤¾à¤œà¤¾à¤° à¤®à¥‡à¤‚ à¤¨à¤¿à¤µà¥‡à¤¶ à¤•à¤°à¤¤à¥‡ à¤¸à¤®à¤¯ à¤•à¤¿à¤¨ à¤¬à¤¾à¤¤à¥‹à¤‚ à¤•à¤¾ à¤§à¥à¤¯à¤¾à¤¨ à¤°à¤–à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤?\"\nresult = test_model(test_instruction, test_input, max_new_tokens=526)\nprint(f\"Input: {test_input}\")\nprint(f\"Output: {result}\")\n\nprint(f\"\\n{'='*80}\")\nprint(f\"âœ… ALL DONE!\")\nprint(f\"{'='*80}\")\nprint(f\"\"\"\nNext steps:\n-----------\n1. Download the model from Kaggle:\n   - Go to Output tab\n   - Download {OUTPUT_DIR} folder\n   \n2. Evaluate on test sets:\n   - test_sentiment_only.json\n   - test_qa_only.json\n   \n# 3. Upload to HuggingFace Hub (optional):\n#    - Use model.push_to_hub()\n   \n# 4. For Telugu: Repeat with Telugu translated data\n\n# Model is ready to use! ğŸ‰\n# \"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:07:26.877422Z","iopub.execute_input":"2025-11-25T09:07:26.878185Z"}},"outputs":[{"name":"stdout","text":"Creating dataset_config.json...\nâœ“ Created dataset_config.json with MAX_LENGTH=1536\nSystem Information:\nPython: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPlatform: linux\nâœ“ CUDA available\nDetected CUDA version: 12.4\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2023.10.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes-cuda118 as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\nInstalling bitsandbytes with CUDA support...\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hâœ“ bitsandbytes installed\nâœ“ Installation complete\n\nâœ“ CUDA environment configured\n  BNB_CUDA_VERSION: 124\n","output_type":"stream"},{"name":"stderr","text":"2025-11-25 09:08:15.372565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764061695.589452      82 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764061695.650093      82 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA device: Tesla T4\nCUDA memory: 15.83 GB\nâœ“ Loaded configuration from dataset_config.json (OVERRIDDEN)\n\n================================================================================\nCONFIGURATION\n================================================================================\nModel: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nOutput directory: ./fingpt-hindi-tinyllama\nMax sequence length: 512\n\nâš ï¸  Using hardcoded token (not recommended for production)\n\n================================================================================\nSETTING UP 4-BIT QUANTIZATION\n================================================================================\nâœ“ Quantization config created\n  - 4-bit precision (NF4)\n  - Double quantization enabled\n  - Compute dtype: bfloat16\n\n================================================================================\nLOADING TOKENIZER\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7be87e817dd423a804a78cdfbfc67da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b317c55f85401896ab2bd9034e4f3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"097c45ddc4954fe280aefc8db32cb627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beef173f1bed4b69af9c89c3596a3666"}},"metadata":{}},{"name":"stdout","text":"âœ“ Tokenizer loaded\n  - Vocabulary size: 32,000\n  - Pad token: </s>\n  - EOS token: </s>\n\n================================================================================\nLOADING BASE MODEL (Single GPU)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5107a03d13af4039b0357631b656cd15"}},"metadata":{}},{"name":"stderr","text":"WARNING:bitsandbytes.cextension:WARNING: BNB_CUDA_VERSION=124 environment variable detected; loading libbitsandbytes_cuda124.so.\nThis can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\nIf this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11da5b3d914146caab7e6adae11a0d7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bdf6d79cb064c6d92dba5297547215d"}},"metadata":{}},{"name":"stdout","text":"âœ“ Model loaded successfully\n  - GPU: Tesla T4\n  - Memory: 15.83 GB\n\n================================================================================\nPREPARING MODEL FOR TRAINING\n================================================================================\nâœ“ Model prepared for k-bit training\nâœ“ use_cache disabled for gradient checkpointing\n\n================================================================================\nCONFIGURING LORA ADAPTERS\n================================================================================\nâœ“ LoRA adapters configured\ntrainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n\n================================================================================\nSETTING UP DATA PROCESSING\n================================================================================\nâœ“ Data processing functions defined (using instruction_hi/input_hi/output_hi)\n\n================================================================================\nLOADING DATASETS (FAST TRAINING MODE)\n================================================================================\n\nâœ“ Using subset for fast training:\n  - Train: 38,603 samples\n  - Validation: 5,188 samples\n\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing train set:   0%|          | 0/38603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b9101331e3b4de09795526108c29ce8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing validation set:   0%|          | 0/5188 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed9203915944a9bb1ad03a90f83fea6"}},"metadata":{}},{"name":"stdout","text":"âœ“ Tokenization complete\n\nğŸ“Š Training estimates:\n  - Steps per epoch: 1206\n  - Total steps: 3618\n  - Estimated time on T4: 2-3 hours â°\n\n================================================================================\nCONFIGURING TRAINING (Optimized for T4)\n================================================================================\nâœ“ Training configuration:\n  - Batch size per device: 8\n  - Gradient accumulation: 1\n  - Effective batch size: 8\n  - Steps per epoch: 4,825\n  - Total steps: 14,475\n\n================================================================================\nINITIALIZING TRAINER\n================================================================================\nâœ“ Trainer initialized\n  - Ready to train!\n\n================================================================================\nSTARTING TRAINING\n================================================================================\n\nâ° Estimated time: 8-12 hours on Kaggle P100\nğŸ“Š Monitor training loss in the output below\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251125_090910-pagnjfuy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iamharshmehtafirst-iit-bombay/huggingface/runs/pagnjfuy' target=\"_blank\">whole-vortex-4</a></strong> to <a href='https://wandb.ai/iamharshmehtafirst-iit-bombay/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iamharshmehtafirst-iit-bombay/huggingface' target=\"_blank\">https://wandb.ai/iamharshmehtafirst-iit-bombay/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iamharshmehtafirst-iit-bombay/huggingface/runs/pagnjfuy' target=\"_blank\">https://wandb.ai/iamharshmehtafirst-iit-bombay/huggingface/runs/pagnjfuy</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8064' max='14478' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 8064/14478 5:41:45 < 4:31:53, 0.39 it/s, Epoch 1.67/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.416900</td>\n      <td>0.413193</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import shutil, os\n\nSRC = \"/kaggle/input/epoch1\"\nOUTPUT_DIR = \"./fingpt-hindi-tinyllama/checkpoint-epoch1\"   # writable location\n\n# create dest if not exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# copy all files\nfor f in os.listdir(SRC):\n    srcf = os.path.join(SRC, f)\n    dstf = os.path.join(OUTPUT_DIR, f)\n    # if it's a file, copy; if a folder, copytree\n    if os.path.isfile(srcf):\n        shutil.copy2(srcf, dstf)\n    else:\n        shutil.copytree(srcf, dstf, dirs_exist_ok=True)\n\nprint(\"Copied checkpoint to:\", OUTPUT_DIR)\nprint(\"Files:\", os.listdir(OUTPUT_DIR))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}